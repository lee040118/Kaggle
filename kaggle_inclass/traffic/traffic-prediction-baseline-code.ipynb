{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Inclass : Sequential\n",
    "베이스라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (8.1.0)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6350 sha256=96ee5ec801e9d5608c27bd663373a3fbd79fb9e1319232332c58ef5481cd7573\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/e0/e9/305e348717e399665119bd012510d51ff4f22d709ff60c3096\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 1.0 MB/s eta 0:00:01    |▏                               | 30 kB 1.1 MB/s eta 0:00:06     |███▎                            | 583 kB 1.0 MB/s eta 0:00:06     |█████▉                          | 1.1 MB 1.0 MB/s eta 0:00:05     |████████████████████████        | 4.3 MB 1.0 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (1.19.2)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.18.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (50.3.1.post20201107)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (0.35.1)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 6.4 MB/s eta 0:00:01     |█                               | 112 kB 6.4 MB/s eta 0:00:01     |██████▍                         | 788 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 38.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.24.0)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 27.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio>=1.24.3->tensorboard) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 31.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: protobuf, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard-data-server, werkzeug, absl-py, tensorboard-plugin-wit, tensorboard\n",
      "Successfully installed absl-py-0.15.0 cachetools-4.2.4 google-auth-2.3.0 google-auth-oauthlib-0.4.6 grpcio-1.41.0 markdown-3.3.4 oauthlib-3.1.1 protobuf-3.18.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 werkzeug-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "import easydict\n",
    "from torch import autograd\n",
    "\n",
    "os.makedirs('log', exist_ok=True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('log/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2021\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/USER/kaggle/traffic/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200101</td>\n",
       "      <td>0</td>\n",
       "      <td>83247</td>\n",
       "      <td>19128</td>\n",
       "      <td>2611</td>\n",
       "      <td>5161</td>\n",
       "      <td>1588</td>\n",
       "      <td>892</td>\n",
       "      <td>32263</td>\n",
       "      <td>1636</td>\n",
       "      <td>...</td>\n",
       "      <td>1311</td>\n",
       "      <td>3482</td>\n",
       "      <td>11299</td>\n",
       "      <td>7072</td>\n",
       "      <td>1176</td>\n",
       "      <td>3810</td>\n",
       "      <td>748</td>\n",
       "      <td>3920</td>\n",
       "      <td>2133</td>\n",
       "      <td>3799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200101</td>\n",
       "      <td>1</td>\n",
       "      <td>89309</td>\n",
       "      <td>19027</td>\n",
       "      <td>3337</td>\n",
       "      <td>5502</td>\n",
       "      <td>1650</td>\n",
       "      <td>1043</td>\n",
       "      <td>35609</td>\n",
       "      <td>1644</td>\n",
       "      <td>...</td>\n",
       "      <td>1162</td>\n",
       "      <td>3849</td>\n",
       "      <td>13180</td>\n",
       "      <td>8771</td>\n",
       "      <td>1283</td>\n",
       "      <td>3763</td>\n",
       "      <td>782</td>\n",
       "      <td>3483</td>\n",
       "      <td>2057</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200101</td>\n",
       "      <td>2</td>\n",
       "      <td>66611</td>\n",
       "      <td>14710</td>\n",
       "      <td>2970</td>\n",
       "      <td>4631</td>\n",
       "      <td>1044</td>\n",
       "      <td>921</td>\n",
       "      <td>26821</td>\n",
       "      <td>1104</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>2299</td>\n",
       "      <td>7986</td>\n",
       "      <td>5426</td>\n",
       "      <td>1536</td>\n",
       "      <td>3229</td>\n",
       "      <td>491</td>\n",
       "      <td>2634</td>\n",
       "      <td>1526</td>\n",
       "      <td>3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200101</td>\n",
       "      <td>3</td>\n",
       "      <td>53290</td>\n",
       "      <td>13753</td>\n",
       "      <td>2270</td>\n",
       "      <td>4242</td>\n",
       "      <td>1021</td>\n",
       "      <td>790</td>\n",
       "      <td>21322</td>\n",
       "      <td>909</td>\n",
       "      <td>...</td>\n",
       "      <td>632</td>\n",
       "      <td>1716</td>\n",
       "      <td>5703</td>\n",
       "      <td>3156</td>\n",
       "      <td>1104</td>\n",
       "      <td>2882</td>\n",
       "      <td>431</td>\n",
       "      <td>2488</td>\n",
       "      <td>1268</td>\n",
       "      <td>3686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200101</td>\n",
       "      <td>4</td>\n",
       "      <td>52095</td>\n",
       "      <td>17615</td>\n",
       "      <td>2406</td>\n",
       "      <td>3689</td>\n",
       "      <td>1840</td>\n",
       "      <td>922</td>\n",
       "      <td>22711</td>\n",
       "      <td>1354</td>\n",
       "      <td>...</td>\n",
       "      <td>875</td>\n",
       "      <td>2421</td>\n",
       "      <td>5816</td>\n",
       "      <td>2933</td>\n",
       "      <td>1206</td>\n",
       "      <td>2433</td>\n",
       "      <td>499</td>\n",
       "      <td>2952</td>\n",
       "      <td>1927</td>\n",
       "      <td>5608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>20200517</td>\n",
       "      <td>19</td>\n",
       "      <td>311727</td>\n",
       "      <td>101285</td>\n",
       "      <td>10085</td>\n",
       "      <td>30637</td>\n",
       "      <td>10060</td>\n",
       "      <td>8749</td>\n",
       "      <td>148935</td>\n",
       "      <td>6801</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>15431</td>\n",
       "      <td>25597</td>\n",
       "      <td>14292</td>\n",
       "      <td>9300</td>\n",
       "      <td>22238</td>\n",
       "      <td>3786</td>\n",
       "      <td>16936</td>\n",
       "      <td>10729</td>\n",
       "      <td>20194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>20200517</td>\n",
       "      <td>20</td>\n",
       "      <td>305354</td>\n",
       "      <td>91426</td>\n",
       "      <td>8607</td>\n",
       "      <td>26021</td>\n",
       "      <td>8095</td>\n",
       "      <td>7198</td>\n",
       "      <td>136503</td>\n",
       "      <td>6147</td>\n",
       "      <td>...</td>\n",
       "      <td>5501</td>\n",
       "      <td>15378</td>\n",
       "      <td>24661</td>\n",
       "      <td>14747</td>\n",
       "      <td>8239</td>\n",
       "      <td>20604</td>\n",
       "      <td>3203</td>\n",
       "      <td>15018</td>\n",
       "      <td>9767</td>\n",
       "      <td>17962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>20200517</td>\n",
       "      <td>21</td>\n",
       "      <td>306008</td>\n",
       "      <td>75113</td>\n",
       "      <td>6325</td>\n",
       "      <td>19933</td>\n",
       "      <td>5711</td>\n",
       "      <td>4494</td>\n",
       "      <td>129412</td>\n",
       "      <td>5134</td>\n",
       "      <td>...</td>\n",
       "      <td>4216</td>\n",
       "      <td>12558</td>\n",
       "      <td>22781</td>\n",
       "      <td>14081</td>\n",
       "      <td>6392</td>\n",
       "      <td>17937</td>\n",
       "      <td>2447</td>\n",
       "      <td>12403</td>\n",
       "      <td>7825</td>\n",
       "      <td>14031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3277</th>\n",
       "      <td>20200517</td>\n",
       "      <td>22</td>\n",
       "      <td>237447</td>\n",
       "      <td>49498</td>\n",
       "      <td>4209</td>\n",
       "      <td>12145</td>\n",
       "      <td>3891</td>\n",
       "      <td>2718</td>\n",
       "      <td>96698</td>\n",
       "      <td>3526</td>\n",
       "      <td>...</td>\n",
       "      <td>2578</td>\n",
       "      <td>8870</td>\n",
       "      <td>16640</td>\n",
       "      <td>11066</td>\n",
       "      <td>4427</td>\n",
       "      <td>11955</td>\n",
       "      <td>1495</td>\n",
       "      <td>7507</td>\n",
       "      <td>5387</td>\n",
       "      <td>8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>20200517</td>\n",
       "      <td>23</td>\n",
       "      <td>150312</td>\n",
       "      <td>27410</td>\n",
       "      <td>2350</td>\n",
       "      <td>6406</td>\n",
       "      <td>1803</td>\n",
       "      <td>1614</td>\n",
       "      <td>55788</td>\n",
       "      <td>1849</td>\n",
       "      <td>...</td>\n",
       "      <td>1377</td>\n",
       "      <td>5021</td>\n",
       "      <td>10058</td>\n",
       "      <td>7139</td>\n",
       "      <td>2250</td>\n",
       "      <td>6844</td>\n",
       "      <td>735</td>\n",
       "      <td>4116</td>\n",
       "      <td>3046</td>\n",
       "      <td>4606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3279 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            날짜  시간      10     100    101    120    121   140     150   160  \\\n",
       "0     20200101   0   83247   19128   2611   5161   1588   892   32263  1636   \n",
       "1     20200101   1   89309   19027   3337   5502   1650  1043   35609  1644   \n",
       "2     20200101   2   66611   14710   2970   4631   1044   921   26821  1104   \n",
       "3     20200101   3   53290   13753   2270   4242   1021   790   21322   909   \n",
       "4     20200101   4   52095   17615   2406   3689   1840   922   22711  1354   \n",
       "...        ...  ..     ...     ...    ...    ...    ...   ...     ...   ...   \n",
       "3274  20200517  19  311727  101285  10085  30637  10060  8749  148935  6801   \n",
       "3275  20200517  20  305354   91426   8607  26021   8095  7198  136503  6147   \n",
       "3276  20200517  21  306008   75113   6325  19933   5711  4494  129412  5134   \n",
       "3277  20200517  22  237447   49498   4209  12145   3891  2718   96698  3526   \n",
       "3278  20200517  23  150312   27410   2350   6406   1803  1614   55788  1849   \n",
       "\n",
       "      ...  1020   1040   1100   1200  1510   2510  3000   4510   5510   6000  \n",
       "0     ...  1311   3482  11299   7072  1176   3810   748   3920   2133   3799  \n",
       "1     ...  1162   3849  13180   8771  1283   3763   782   3483   2057   4010  \n",
       "2     ...   768   2299   7986   5426  1536   3229   491   2634   1526   3388  \n",
       "3     ...   632   1716   5703   3156  1104   2882   431   2488   1268   3686  \n",
       "4     ...   875   2421   5816   2933  1206   2433   499   2952   1927   5608  \n",
       "...   ...   ...    ...    ...    ...   ...    ...   ...    ...    ...    ...  \n",
       "3274  ...  6726  15431  25597  14292  9300  22238  3786  16936  10729  20194  \n",
       "3275  ...  5501  15378  24661  14747  8239  20604  3203  15018   9767  17962  \n",
       "3276  ...  4216  12558  22781  14081  6392  17937  2447  12403   7825  14031  \n",
       "3277  ...  2578   8870  16640  11066  4427  11955  1495   7507   5387   8889  \n",
       "3278  ...  1377   5021  10058   7139  2250   6844   735   4116   3046   4606  \n",
       "\n",
       "[3279 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200511</td>\n",
       "      <td>0</td>\n",
       "      <td>77968</td>\n",
       "      <td>14429</td>\n",
       "      <td>1233</td>\n",
       "      <td>4021</td>\n",
       "      <td>981</td>\n",
       "      <td>881</td>\n",
       "      <td>28672</td>\n",
       "      <td>1064</td>\n",
       "      <td>...</td>\n",
       "      <td>637</td>\n",
       "      <td>2604</td>\n",
       "      <td>5239</td>\n",
       "      <td>4168</td>\n",
       "      <td>1155</td>\n",
       "      <td>3596</td>\n",
       "      <td>337</td>\n",
       "      <td>2262</td>\n",
       "      <td>1608</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200511</td>\n",
       "      <td>1</td>\n",
       "      <td>48679</td>\n",
       "      <td>9136</td>\n",
       "      <td>823</td>\n",
       "      <td>2618</td>\n",
       "      <td>654</td>\n",
       "      <td>572</td>\n",
       "      <td>17722</td>\n",
       "      <td>672</td>\n",
       "      <td>...</td>\n",
       "      <td>353</td>\n",
       "      <td>1870</td>\n",
       "      <td>3359</td>\n",
       "      <td>2558</td>\n",
       "      <td>1002</td>\n",
       "      <td>2157</td>\n",
       "      <td>257</td>\n",
       "      <td>1425</td>\n",
       "      <td>1018</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200511</td>\n",
       "      <td>2</td>\n",
       "      <td>33773</td>\n",
       "      <td>8199</td>\n",
       "      <td>578</td>\n",
       "      <td>2188</td>\n",
       "      <td>392</td>\n",
       "      <td>502</td>\n",
       "      <td>14464</td>\n",
       "      <td>579</td>\n",
       "      <td>...</td>\n",
       "      <td>345</td>\n",
       "      <td>1499</td>\n",
       "      <td>2646</td>\n",
       "      <td>2022</td>\n",
       "      <td>876</td>\n",
       "      <td>1959</td>\n",
       "      <td>232</td>\n",
       "      <td>1155</td>\n",
       "      <td>927</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200511</td>\n",
       "      <td>3</td>\n",
       "      <td>41511</td>\n",
       "      <td>9986</td>\n",
       "      <td>726</td>\n",
       "      <td>2817</td>\n",
       "      <td>555</td>\n",
       "      <td>646</td>\n",
       "      <td>17793</td>\n",
       "      <td>650</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>1730</td>\n",
       "      <td>3398</td>\n",
       "      <td>1967</td>\n",
       "      <td>912</td>\n",
       "      <td>2462</td>\n",
       "      <td>281</td>\n",
       "      <td>1477</td>\n",
       "      <td>959</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200511</td>\n",
       "      <td>4</td>\n",
       "      <td>78680</td>\n",
       "      <td>19509</td>\n",
       "      <td>1463</td>\n",
       "      <td>4720</td>\n",
       "      <td>825</td>\n",
       "      <td>1088</td>\n",
       "      <td>35125</td>\n",
       "      <td>997</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>2958</td>\n",
       "      <td>7369</td>\n",
       "      <td>4120</td>\n",
       "      <td>1569</td>\n",
       "      <td>4568</td>\n",
       "      <td>577</td>\n",
       "      <td>3155</td>\n",
       "      <td>1871</td>\n",
       "      <td>3656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200524</td>\n",
       "      <td>19</td>\n",
       "      <td>314226</td>\n",
       "      <td>98345</td>\n",
       "      <td>10625</td>\n",
       "      <td>28618</td>\n",
       "      <td>8316</td>\n",
       "      <td>6684</td>\n",
       "      <td>141675</td>\n",
       "      <td>6619</td>\n",
       "      <td>...</td>\n",
       "      <td>8254</td>\n",
       "      <td>16118</td>\n",
       "      <td>23304</td>\n",
       "      <td>14082</td>\n",
       "      <td>8447</td>\n",
       "      <td>21694</td>\n",
       "      <td>2180</td>\n",
       "      <td>15746</td>\n",
       "      <td>10903</td>\n",
       "      <td>21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200524</td>\n",
       "      <td>20</td>\n",
       "      <td>300001</td>\n",
       "      <td>87871</td>\n",
       "      <td>8226</td>\n",
       "      <td>22706</td>\n",
       "      <td>6981</td>\n",
       "      <td>5743</td>\n",
       "      <td>142933</td>\n",
       "      <td>6295</td>\n",
       "      <td>...</td>\n",
       "      <td>5225</td>\n",
       "      <td>15297</td>\n",
       "      <td>21919</td>\n",
       "      <td>14526</td>\n",
       "      <td>7332</td>\n",
       "      <td>19732</td>\n",
       "      <td>1990</td>\n",
       "      <td>14096</td>\n",
       "      <td>10028</td>\n",
       "      <td>17787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200524</td>\n",
       "      <td>21</td>\n",
       "      <td>304150</td>\n",
       "      <td>71126</td>\n",
       "      <td>6002</td>\n",
       "      <td>18317</td>\n",
       "      <td>4939</td>\n",
       "      <td>3779</td>\n",
       "      <td>133110</td>\n",
       "      <td>4781</td>\n",
       "      <td>...</td>\n",
       "      <td>4072</td>\n",
       "      <td>12685</td>\n",
       "      <td>21135</td>\n",
       "      <td>14403</td>\n",
       "      <td>5443</td>\n",
       "      <td>16967</td>\n",
       "      <td>1359</td>\n",
       "      <td>11670</td>\n",
       "      <td>7963</td>\n",
       "      <td>14041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200524</td>\n",
       "      <td>22</td>\n",
       "      <td>236751</td>\n",
       "      <td>44947</td>\n",
       "      <td>3575</td>\n",
       "      <td>11455</td>\n",
       "      <td>3135</td>\n",
       "      <td>2536</td>\n",
       "      <td>98582</td>\n",
       "      <td>3267</td>\n",
       "      <td>...</td>\n",
       "      <td>2489</td>\n",
       "      <td>8093</td>\n",
       "      <td>14427</td>\n",
       "      <td>10914</td>\n",
       "      <td>3861</td>\n",
       "      <td>11397</td>\n",
       "      <td>859</td>\n",
       "      <td>7270</td>\n",
       "      <td>5194</td>\n",
       "      <td>8230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200524</td>\n",
       "      <td>23</td>\n",
       "      <td>143609</td>\n",
       "      <td>26137</td>\n",
       "      <td>2242</td>\n",
       "      <td>6166</td>\n",
       "      <td>1609</td>\n",
       "      <td>1391</td>\n",
       "      <td>54633</td>\n",
       "      <td>1899</td>\n",
       "      <td>...</td>\n",
       "      <td>1343</td>\n",
       "      <td>4686</td>\n",
       "      <td>8732</td>\n",
       "      <td>6986</td>\n",
       "      <td>2161</td>\n",
       "      <td>6487</td>\n",
       "      <td>410</td>\n",
       "      <td>3963</td>\n",
       "      <td>2686</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간      10    100    101    120   121   140     150   160  ...  \\\n",
       "0    20200511   0   77968  14429   1233   4021   981   881   28672  1064  ...   \n",
       "1    20200511   1   48679   9136    823   2618   654   572   17722   672  ...   \n",
       "2    20200511   2   33773   8199    578   2188   392   502   14464   579  ...   \n",
       "3    20200511   3   41511   9986    726   2817   555   646   17793   650  ...   \n",
       "4    20200511   4   78680  19509   1463   4720   825  1088   35125   997  ...   \n",
       "..        ...  ..     ...    ...    ...    ...   ...   ...     ...   ...  ...   \n",
       "331  20200524  19  314226  98345  10625  28618  8316  6684  141675  6619  ...   \n",
       "332  20200524  20  300001  87871   8226  22706  6981  5743  142933  6295  ...   \n",
       "333  20200524  21  304150  71126   6002  18317  4939  3779  133110  4781  ...   \n",
       "334  20200524  22  236751  44947   3575  11455  3135  2536   98582  3267  ...   \n",
       "335  20200524  23  143609  26137   2242   6166  1609  1391   54633  1899  ...   \n",
       "\n",
       "     1020   1040   1100   1200  1510   2510  3000   4510   5510   6000  \n",
       "0     637   2604   5239   4168  1155   3596   337   2262   1608   2337  \n",
       "1     353   1870   3359   2558  1002   2157   257   1425   1018   1810  \n",
       "2     345   1499   2646   2022   876   1959   232   1155    927   1530  \n",
       "3     390   1730   3398   1967   912   2462   281   1477    959   1882  \n",
       "4     679   2958   7369   4120  1569   4568   577   3155   1871   3656  \n",
       "..    ...    ...    ...    ...   ...    ...   ...    ...    ...    ...  \n",
       "331  8254  16118  23304  14082  8447  21694  2180  15746  10903  21014  \n",
       "332  5225  15297  21919  14526  7332  19732  1990  14096  10028  17787  \n",
       "333  4072  12685  21135  14403  5443  16967  1359  11670   7963  14041  \n",
       "334  2489   8093  14427  10914  3861  11397   859   7270   5194   8230  \n",
       "335  1343   4686   8732   6986  2161   6487   410   3963   2686   4690  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200518</td>\n",
       "      <td>0</td>\n",
       "      <td>82065</td>\n",
       "      <td>15172</td>\n",
       "      <td>1500</td>\n",
       "      <td>3294</td>\n",
       "      <td>1086</td>\n",
       "      <td>962</td>\n",
       "      <td>28931</td>\n",
       "      <td>1103</td>\n",
       "      <td>...</td>\n",
       "      <td>618</td>\n",
       "      <td>2790</td>\n",
       "      <td>5147</td>\n",
       "      <td>4331</td>\n",
       "      <td>1329</td>\n",
       "      <td>3665</td>\n",
       "      <td>404</td>\n",
       "      <td>2242</td>\n",
       "      <td>1619</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200518</td>\n",
       "      <td>1</td>\n",
       "      <td>51248</td>\n",
       "      <td>9840</td>\n",
       "      <td>813</td>\n",
       "      <td>2356</td>\n",
       "      <td>696</td>\n",
       "      <td>546</td>\n",
       "      <td>17888</td>\n",
       "      <td>720</td>\n",
       "      <td>...</td>\n",
       "      <td>430</td>\n",
       "      <td>1864</td>\n",
       "      <td>3269</td>\n",
       "      <td>2561</td>\n",
       "      <td>921</td>\n",
       "      <td>2081</td>\n",
       "      <td>272</td>\n",
       "      <td>1390</td>\n",
       "      <td>1003</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200518</td>\n",
       "      <td>2</td>\n",
       "      <td>39026</td>\n",
       "      <td>7894</td>\n",
       "      <td>760</td>\n",
       "      <td>2413</td>\n",
       "      <td>408</td>\n",
       "      <td>549</td>\n",
       "      <td>13357</td>\n",
       "      <td>498</td>\n",
       "      <td>...</td>\n",
       "      <td>322</td>\n",
       "      <td>1313</td>\n",
       "      <td>2765</td>\n",
       "      <td>1931</td>\n",
       "      <td>920</td>\n",
       "      <td>1764</td>\n",
       "      <td>228</td>\n",
       "      <td>1136</td>\n",
       "      <td>922</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200518</td>\n",
       "      <td>3</td>\n",
       "      <td>40993</td>\n",
       "      <td>10137</td>\n",
       "      <td>780</td>\n",
       "      <td>2701</td>\n",
       "      <td>420</td>\n",
       "      <td>741</td>\n",
       "      <td>15544</td>\n",
       "      <td>532</td>\n",
       "      <td>...</td>\n",
       "      <td>326</td>\n",
       "      <td>1766</td>\n",
       "      <td>3320</td>\n",
       "      <td>2060</td>\n",
       "      <td>892</td>\n",
       "      <td>2447</td>\n",
       "      <td>337</td>\n",
       "      <td>1495</td>\n",
       "      <td>975</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200518</td>\n",
       "      <td>4</td>\n",
       "      <td>77863</td>\n",
       "      <td>19603</td>\n",
       "      <td>1276</td>\n",
       "      <td>5019</td>\n",
       "      <td>968</td>\n",
       "      <td>1160</td>\n",
       "      <td>32101</td>\n",
       "      <td>968</td>\n",
       "      <td>...</td>\n",
       "      <td>669</td>\n",
       "      <td>2914</td>\n",
       "      <td>6986</td>\n",
       "      <td>3911</td>\n",
       "      <td>1368</td>\n",
       "      <td>4380</td>\n",
       "      <td>513</td>\n",
       "      <td>2940</td>\n",
       "      <td>1758</td>\n",
       "      <td>3629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200531</td>\n",
       "      <td>19</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200531</td>\n",
       "      <td>20</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200531</td>\n",
       "      <td>21</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200531</td>\n",
       "      <td>22</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200531</td>\n",
       "      <td>23</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간     10    100   101   120   121   140    150   160  ...  \\\n",
       "0    20200518   0  82065  15172  1500  3294  1086   962  28931  1103  ...   \n",
       "1    20200518   1  51248   9840   813  2356   696   546  17888   720  ...   \n",
       "2    20200518   2  39026   7894   760  2413   408   549  13357   498  ...   \n",
       "3    20200518   3  40993  10137   780  2701   420   741  15544   532  ...   \n",
       "4    20200518   4  77863  19603  1276  5019   968  1160  32101   968  ...   \n",
       "..        ...  ..    ...    ...   ...   ...   ...   ...    ...   ...  ...   \n",
       "331  20200531  19   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "332  20200531  20   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "333  20200531  21   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "334  20200531  22   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "335  20200531  23   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "\n",
       "     1020  1040  1100  1200  1510  2510  3000  4510  5510  6000  \n",
       "0     618  2790  5147  4331  1329  3665   404  2242  1619  2314  \n",
       "1     430  1864  3269  2561   921  2081   272  1390  1003  1766  \n",
       "2     322  1313  2765  1931   920  1764   228  1136   922  1309  \n",
       "3     326  1766  3320  2060   892  2447   337  1495   975  1912  \n",
       "4     669  2914  6986  3911  1368  4380   513  2940  1758  3629  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "331  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "332  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "333  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "334  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "335  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "* 한 칼럼에 대한 7일(168행) 데이터를 input_data, 뒤따르는 7일 데이터를 output_data로 반환합니다.\n",
    "* 도로별 차이를 두지 않고 모든 도로를 동일한 타입의 데이터로 취급합니다.\n",
    "* 모든 csv 파일의 마지막 168행은 예측해야하는 값이므로 input으로 들어가지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, root, seq_len, batch_size=64, phase='train'):\n",
    "        self.root = root\n",
    "        self.phase = phase\n",
    "        self.seq_len = seq_len * 24\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "\n",
    "        self.label_path = os.path.join(self.root, self.phase + '.csv')\n",
    "\n",
    "        df = pd.read_csv(self.label_path)\n",
    "        timestamps = [(i, j) for (i, j) in zip(list(df['날짜']), list(df['시간']))]\n",
    "        categories = df.columns.values.tolist()[2:]\n",
    "\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for t in range(len(timestamps)):\n",
    "            temp_input_data = []\n",
    "            temp_output_data = []\n",
    "            for col in categories:\n",
    "                road = df[col].tolist()\n",
    "                inp = [float(i) for i in road[t:t+self.seq_len]]\n",
    "                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]\n",
    "                temp_input_data.append(inp)\n",
    "                temp_output_data.append(outp)\n",
    "            input_data.append(temp_input_data)\n",
    "            output_data.append(temp_output_data)\n",
    "\n",
    "        self.labels['timestamp'] = timestamps\n",
    "        self.labels['category'] = categories\n",
    "        self.labels['input'] = input_data\n",
    "        self.labels['output'] = output_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row = index // 35\n",
    "        col = index % 35\n",
    "\n",
    "        timestamp = self.labels['timestamp'][row]\n",
    "        category = self.labels['category'][col]\n",
    "        \n",
    "        input_data = torch.tensor(self.labels['input'][row][col])\n",
    "\n",
    "        if self.phase != 'test':\n",
    "            output_data = torch.tensor(self.labels['output'][row][col])\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        return timestamp, category, (input_data, output_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels['timestamp']) - (self.seq_len * 2) + 1) * 35\n",
    "\n",
    "    def get_label_file(self):\n",
    "        return self.label_path\n",
    "\n",
    "\n",
    "def data_loader(root, phase='train', batch_size=64, seq_len=7, drop_last=False):\n",
    "    if phase == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CustomDataset(root, seq_len, batch_size, phase)\n",
    "    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader, dataset.get_label_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=168,\n",
    "                 hidden_size=1024,\n",
    "                 output_size=168,\n",
    "\n",
    "                 batch_size=64,\n",
    "\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 batch_first=False):\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            dropout=0.2,\n",
    "                            num_layers=num_layers)\n",
    "\n",
    "        ##### Layer 2\n",
    "        self.lstm2 = nn.LSTM(hidden_size, \n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        ##### Finalize\n",
    "        self.linear = nn.Linear(hidden_size, \n",
    "                                output_size)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_in, c_in):\n",
    "\n",
    "        h_in = nn.Parameter(h_in.type(dtype), requires_grad=True)\n",
    "        c_in = nn.Parameter(c_in.type(dtype), requires_grad=True)\n",
    "\n",
    "        # Layer 1\n",
    "        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Layer2\n",
    "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Final\n",
    "        predictions = self.linear(lstm_out)\n",
    "        \n",
    "        return predictions, (h_2, c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer, scheduler):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join('log', model_name + '.pth'))\n",
    "    print('model saved')\n",
    "    return os.path.join('log', model_name + '.pth')\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None, scheduler=None):\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "model_name = 'sequential'\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "print_iter = 200\n",
    "val_epoch = 1\n",
    "save_epoch = 1\n",
    "base_lr = 0.01\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "output_size = input_size\n",
    "hidden_size = 1024\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 파일과 모델 가중치 파일 저장을 위해 log 디렉토리가 생성됩니다. 중요한 파일이 덮어씌워지지 않도록 주의 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('log', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = RMSLELoss()\n",
    "\n",
    "# optimizer & scheduler\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "num of parameters :  97427624\n",
      "num of trainable parameters : 97427624\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get data loader\n",
    "train_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                  phase='train',\n",
    "                                  batch_size=batch_size,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)\n",
    "\n",
    "validate_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                     phase='validate',\n",
    "                                     batch_size=1,\n",
    "                                     seq_len=seq_len,\n",
    "                                     drop_last=True)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"num of parameters : \",total_params)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"num of trainable parameters :\", trainable_params)\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 168])\n",
      "Epoch:     0 | Iteration:     0 | Loss: 5.52821\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n",
      "torch.Size([1, 64, 168])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35585/1369209362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "epoch_loss, min_epoch_loss = 0.0, 99.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for iter_, sample in enumerate(train_dataloader):\n",
    "\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device),\n",
    "                        torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device))\n",
    "\n",
    "        _, _, (input_data, output_data) = sample\n",
    "        input_data = input_data.unsqueeze(0).to(device)\n",
    "        output_data = output_data.unsqueeze(0).to(device)\n",
    "        print(input_data.size())\n",
    "        pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "        \n",
    "        loss = criterion(pred, output_data)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if iter_ % 200 == 199:\n",
    "            writer.add_scalar('training loss',\n",
    "                    running_loss / 200,\n",
    "                    epoch * len(train_dataloader) + iter_)\n",
    "            running_loss = 0\n",
    "\n",
    "        if iter_ % print_iter == 0:\n",
    "            print('Epoch: {:5} | Iteration: {:5} | Loss: {:1.5f}'.format(epoch, iter_, loss))\n",
    "\n",
    "    scheduler.step()\n",
    "    print('\\nEpoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, sum(avg_loss) / len(avg_loss)))\n",
    "    avg_loss = []\n",
    "    \n",
    "    if epoch_loss < min_epoch_loss:\n",
    "        save_model('best', model, optimizer, scheduler)\n",
    "        print(f'best model so far: epoch {epoch}')\n",
    "        min_epoch_loss = epoch_loss\n",
    "\n",
    "    if epoch % save_epoch == 0:\n",
    "        save_model('last', model, optimizer, scheduler)\n",
    "        print(f'last model: epoch {epoch}')\n",
    "\n",
    "    if epoch % val_epoch == 0:\n",
    "        \n",
    "        val_score = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                    torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "            for iter_, sample in enumerate(validate_dataloader):\n",
    "\n",
    "                _, _, (input_data, output_data) = sample\n",
    "\n",
    "                input_data = input_data.unsqueeze(0).to(device)\n",
    "                output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "                pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "                score = criterion(pred, output_data)\n",
    "                val_score += score.item()\n",
    "        \n",
    "        print('\\nValidation Epoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, val_score/len(validate_dataloader)))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 1\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader, _ = data_loader(root=DATASET_PATH,\n",
    "                                  phase='test',\n",
    "                                  batch_size=batch_size,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "# model\n",
    "model_name = 'log/best.pth'\n",
    "\n",
    "load_model(model_name, model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "for iter_, sample in enumerate(test_dataloader):\n",
    "\n",
    "    timestamp, category, (input_data, output_data) = sample\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "\n",
    "    pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "\n",
    "    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n",
    "        for cat, row in zip(category, pred[0]):\n",
    "            cat = f'road_{cat}'\n",
    "            submission_table[cat] = row.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 셀을 실행하면 기존에 있던 prediction.csv가 덮어씌워집니다. 필요하시면 코드를 수정하거나 혹은 prediction.csv를 백업하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
