{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979c57c3-5e58-4a9e-a3fa-4897b53d156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import logging\n",
    "from kobart_transformers import get_kobart_tokenizer\n",
    "from kobart_transformers import get_kobart_for_conditional_generation\n",
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ca1019-8a21-4b25-a757-14475d035d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "MODEL_DIR = '/USER/Kaggle/dacon/final_model'\n",
    "ROOT_DIR = '/USER/Kaggle/dacon/final_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28682651-f455-48f5-9453-034b35a544b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train/train_original.json' # 법률\n",
    "train_path2 = './data/train/train_original2.json'\n",
    "train_path3 = './data/train/train_original3.json'\n",
    "valid_path = './data/valid/valid_original.json' \n",
    "valid_path2 = './data/valid/valid_original2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854b1d95-d586-4f4f-a845-a4d90971acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(train_path, 'r')) # 법률\n",
    "# train_data2 = json.load(open(train_path2, 'r'))\n",
    "train_data3 = json.load(open(train_path3, 'r'))\n",
    "valid_data = json.load(open(valid_path2,'r')) # 법률\n",
    "valid_data3 = json.load(open(valid_path,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d0161b-31f3-456b-b0a2-626f5f0caa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datas):\n",
    "    id, original, ext, abs =[], [], [], []\n",
    "    e_pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    pattern = '기자'\n",
    "    for data in tqdm(datas):\n",
    "        id.append(data['id'])\n",
    "        abs.append(data['abstractive'][0])\n",
    "        ext_tmp = ''\n",
    "        for idx in data['extractive']:\n",
    "            for articles in data['text']:\n",
    "                for article in articles:\n",
    "                    if idx == article['index']:\n",
    "                        ext_tmp+= article['sentence'] + ' '\n",
    "        ext.append(ext_tmp[:-1])\n",
    "        text = ''\n",
    "        for articles in data['text']:\n",
    "            for article in articles:\n",
    "                if re.search(pattern=e_pattern, string= article['sentence']) != None: continue\n",
    "                if re.search(pattern=pattern, string= article['sentence']) != None: continue\n",
    "                text += article['sentence'] + ' '\n",
    "        \n",
    "        original.append(text[:-1]) \n",
    "    sum_data = { 'id' : id, 'abs' : abs, 'ext' : ext , 'original' : original}\n",
    "    df = pd.DataFrame(sum_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486975b2-0086-4d4f-afcc-652344c7722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, s_type):\n",
    "    df = data.loc[:,['id','original', s_type]]\n",
    "    df.rename(columns = {s_type : 'summary'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb82536-d740-4d7f-9d0d-780e75b7f916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df60ab0fc89f44af9122fe3d61d8aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3329bcae6b7e42c9880e1b52a08a265c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d323dfb083674ed9a88f8de27ebf9355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4040189169dd475ca2c8ebf217d552a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_t_1 = load_data(train_data3['documents'][50000:70000]) # 문서\n",
    "df_t_2 = load_data(train_data['documents'][1000:8000]) # 법률\n",
    "df_v_1 = load_data(valid_data3['documents'][500:1000]) # 문서\n",
    "df_v_2 = load_data(valid_data['documents'][300:600]) # 법률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb96c88-6b14-4200-99d8-0b30919e4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat([df_t_1,df_t_2], ignore_index=True)\n",
    "df_v = pd.concat([df_v_1,df_v_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40508899-3cd6-404c-b3a8-59c686e647e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abs = data_split(df_t,'abs')\n",
    "valid_abs = data_split(df_v,'abs')\n",
    "trian_ext = data_split(df_t,'ext')\n",
    "valid_ext = data_split(df_v,'ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7318e5b-ec02-4ffe-8187-618ba362df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "class KoBARTSummaryDataset(Dataset):\n",
    "    def __init__(self, df,  pad_index = None, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tok = AutoTokenizer.from_pretrained('hyunwoongko/kobart', use_fast=True)\n",
    "        self.max_len = 1024\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "        if pad_index is None:\n",
    "            self.pad_index = self.tok.pad_token_id\n",
    "        else:\n",
    "            self.pad_index = pad_index\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.df.iloc[idx]\n",
    "        input_ids = self.tok.encode('<s> '+instance['original'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tok.encode(instance['summary'])\n",
    "        label_ids.append(self.tok.eos_token_id)\n",
    "        dec_input_ids = [self.tok.bos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f90c8551-5983-4a79-b57f-8525c9143f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "EVAL_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1193243e-d6be-4d25-9488-33a009156ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_train_dataset = KoBARTSummaryDataset(trian_ext)\n",
    "ext_valid_dataset = KoBARTSummaryDataset(valid_ext)\n",
    "abs_train_dataset = KoBARTSummaryDataset(train_abs)\n",
    "abs_valid_dataset = KoBARTSummaryDataset(valid_abs)\n",
    "\n",
    "ext_train_loader = DataLoader(ext_train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n",
    "ext_valid_loader = DataLoader(ext_valid_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=True,num_workers=0)\n",
    "abs_train_loader = DataLoader(abs_train_dataset, batch_size=TRAIN_BATCH_SIZE ,shuffle=True,num_workers=0)\n",
    "abs_valid_loader = DataLoader(abs_valid_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2af02f-59eb-4119-99fb-411d63d0de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hitrate(y_true, y_pred):\n",
    "    m = datasets.load_metric('rouge')\n",
    "    rouge = m.compute(predictions= y_pred,references=y_true)\n",
    "    score = (rouge['rouge1'].mid.fmeasure+rouge['rouge2'].mid.fmeasure + rouge['rougeL'].mid.fmeasure) /3 \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad26539-39d2-45d0-9863-aff245b6cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \n",
    "    Attributes:\n",
    "        model (`model`)\n",
    "        device (str)\n",
    "        loss_fn (Callable)\n",
    "        metric_fn (Callable)\n",
    "        optimizer (`optimizer`)\n",
    "        scheduler (`scheduler`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model,device,metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        # self.loss_fn = loss_fn\n",
    "        self.tok =  AutoTokenizer.from_pretrained('hyunwoongko/kobart', use_fast=True)\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.logger = logger\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.train_total_loss = 0\n",
    "        total_loss = 0\n",
    "        # pred_lst = []\n",
    "        # target_lst = []\n",
    "        for batch_index, data in enumerate(tqdm(dataloader)):\n",
    "            attention_mask = data['input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "            decoder_attention_mask = data['decoder_input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "            outputs = self.model(input_ids=data['input_ids'].to(device),\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  decoder_input_ids=data['decoder_input_ids'].to(device),\n",
    "                                  decoder_attention_mask=decoder_attention_mask,\n",
    "                                  labels=data['labels'].to(device), return_dict=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            self.train_total_loss += outputs.loss\n",
    "            total_loss += outputs.loss\n",
    "            outputs.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            if (batch_index+1) % 200 == 0:\n",
    "                print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}'.format\n",
    "                      (epoch_index , epoch_index, (batch_index+1) , total_loss / 200))\n",
    "\n",
    "                total_loss = 0\n",
    "                \n",
    "        self.train_mean_loss = self.train_total_loss / len(dataloader)\n",
    "    \n",
    "        msg = f'Epoch {epoch_index}, Train, loss: {self.train_mean_loss}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.val_score_all = 0\n",
    "        pred_lst = []\n",
    "        target_lst = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, data in enumerate(tqdm(dataloader)):\n",
    "                attention_mask = data['input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "                decoder_attention_mask = data['decoder_input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "                outputs = self.model.generate(input_ids=data['input_ids'].to(device),\n",
    "                                  attention_mask = attention_mask,\n",
    "                                     num_beams=5,\n",
    "                                     no_repeat_ngram_size=4,\n",
    "                                     decoder_start_token_id=self.tok.bos_token_id,\n",
    "                                     temperature=1.0, top_k=0, top_p=0.92,\n",
    "                                     length_penalty=1.0, min_length=1,\n",
    "                                     max_length=100,\n",
    "                                     early_stopping=False,\n",
    "                                     num_return_sequences=1,\n",
    "                                     do_sample= True).to(device)\n",
    "                # outputs = self.model(input_ids=data['input_ids'].to(device),\n",
    "                #                   attention_mask=attention_mask,\n",
    "                #                   decoder_input_ids=data['decoder_input_ids'].to(device),\n",
    "                #                   decoder_attention_mask=decoder_attention_mask,\n",
    "                #                   labels=data['labels'].to(device), return_dict=True)\n",
    "                ref = self.tok.batch_decode(\n",
    "                        data['decoder_input_ids'],\n",
    "                        skip_special_tokens=True\n",
    "                )\n",
    "                pred = self.tok.batch_decode(\n",
    "                        outputs,\n",
    "                        skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                # loss = self.loss_fn(sent_score, target)\n",
    "                # self.val_total_loss += outputs.loss\n",
    "            # self.val_mean_loss = self.val_total_loss / len(dataloader)\n",
    "                self.val_score = self.metric_fn(y_true=ref, y_pred=pred)\n",
    "                self.val_score_all += self.val_score\n",
    "            self.val_mean_score = self.val_score_all/len(dataloader)\n",
    "            msg = f'Epoch {epoch_index}, Validation, score : {self.val_mean_score}'\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e45ef15-ca00-49a3-a2e0-9bd1f644db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "ext_epochs = 1\n",
    "abs_epochs = 7\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.00001\n",
    "NUM_WORKERS = 1\n",
    "EARLY_STOPPING_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fefe49a-e2fc-47f9-abc7-b1edfc5d712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_kobart_for_conditional_generation().to(device)\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'final_abs.pt')\n",
    "model = get_kobart_for_conditional_generation().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_DIR)['model_state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, div_factor=1e3, max_lr=3e-5, epochs=abs_epochs, steps_per_epoch=len(abs_train_loader))\n",
    "metric_fn = Hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8746ec-dbdc-4d91-8f51-574eb10a7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model ,device, metric_fn,optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9a5c57-148b-4245-ac73-469bbdc80e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76306cf20c446149de7525bd1c9bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dc97a91d9d44bb8f35afdbdd1dde35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_139708/3720604291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_valid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_139708/820522691.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "criterion = 0\n",
    "\n",
    "for epoch_index in tqdm(range(abs_epochs)):\n",
    "    \n",
    "    trainer.train_epoch(abs_train_loader, epoch_index=epoch_index)\n",
    "    trainer.validate_epoch(abs_valid_loader, epoch_index=epoch_index)\n",
    "   \n",
    "    # early_stopping check\n",
    "#     early_stopper.check_early_stopping(loss=trainer.val_mean_score)\n",
    "\n",
    "#     if early_stopper.stop:\n",
    "#         print('Early stopped')\n",
    "#         break\n",
    "\n",
    "    if trainer.val_mean_score > criterion:\n",
    "        criterion = trainer.val_mean_score     \n",
    "        torch.save({\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trainer.val_mean_score,\n",
    "            }, os.path.join(ROOT_DIR, 'final_abs.pt'))\n",
    "        print('final_abs.pt saved ', epoch_index, trainer.val_mean_score)\n",
    "        \n",
    "        \n",
    "print(\"train finished, best.pt saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
