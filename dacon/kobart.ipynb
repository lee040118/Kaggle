{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f53e38-00ab-4a66-a578-d3b91e82e200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "import logging\n",
    "from kobart_transformers import get_kobart_tokenizer\n",
    "from kobart_transformers import get_kobart_for_conditional_generation\n",
    "import os\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41597946-03ff-45b4-8d88-1a06176c232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "MODEL_DIR = '/USER/Kaggle/dacon/final_model'\n",
    "ROOT_DIR = '/USER/Kaggle/dacon/final_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f2ff34-6629-47e4-8ac4-73bd69a424dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train/train_original.json' # 법률\n",
    "train_path2 = './data/train/train_original2.json'\n",
    "train_path3 = './data/train/train_original3.json'\n",
    "valid_path = './data/valid/valid_original.json' \n",
    "valid_path2 = './data/valid/valid_original2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e5d035-d982-4254-be5f-cc022ec82bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(train_path, 'r')) # 법률\n",
    "# train_data2 = json.load(open(train_path2, 'r'))\n",
    "train_data3 = json.load(open(train_path3, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7060b49d-a58a-41ed-88ba-7447f287c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = json.load(open(valid_path2,'r')) # 법률\n",
    "valid_data3 = json.load(open(valid_path,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2b3f17-ec4b-4347-816d-c4f8af40d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datas):\n",
    "    id, original, ext, abs =[], [], [], []\n",
    "    e_pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    pattern = '기자'\n",
    "    for data in tqdm(datas):\n",
    "        id.append(data['id'])\n",
    "        abs.append(data['abstractive'][0])\n",
    "        ext_tmp = ''\n",
    "        for idx in data['extractive']:\n",
    "            for articles in data['text']:\n",
    "                for article in articles:\n",
    "                    if idx == article['index']:\n",
    "                        ext_tmp+= article['sentence'] + ' '\n",
    "        ext.append(ext_tmp[:-1])\n",
    "        text = ''\n",
    "        for articles in data['text']:\n",
    "            for article in articles:\n",
    "                if re.search(pattern=e_pattern, string= article['sentence']) != None: continue\n",
    "                if re.search(pattern=pattern, string= article['sentence']) != None: continue\n",
    "                text += article['sentence'] + ' '\n",
    "        \n",
    "        original.append(text[:-1]) \n",
    "    sum_data = { 'id' : id, 'abs' : abs, 'ext' : ext , 'original' : original}\n",
    "    df = pd.DataFrame(sum_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0effa942-f741-4434-85d3-21306ec4d53d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f5e3730a964ac5a41189f9b81e5f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3f2a4a8c3d4d59a399ad74a2725f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d303aa54b4c3c9514efe08d787869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f021877c901f4e66979a56a4ae67e553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_t_1 = load_data(train_data3['documents'][50000:70000]) # 문서\n",
    "df_t_2 = load_data(train_data['documents'][1000:8000]) # 법률\n",
    "df_v_1 = load_data(valid_data3['documents'][500:1000]) # 문서\n",
    "df_v_2 = load_data(valid_data['documents'][300:600]) # 법률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39409451-557c-4da6-b3a2-8383b0c469bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat([df_t_1,df_t_2], ignore_index=True)\n",
    "df_v = pd.concat([df_v_1,df_v_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da072c3-6a4b-44c6-9c05-062ce42c3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.18462962962963 844.8179259259259\n"
     ]
    }
   ],
   "source": [
    "o = 0\n",
    "abs =0\n",
    "for i in df_t.loc[:,'abs']:\n",
    "    # print(i)\n",
    "    abs += (len(i))\n",
    "for i in df_t.loc[:,'original']:\n",
    "    o += (len(i))\n",
    "abs = abs/len(df_t)\n",
    "o /= len(df_t)\n",
    "print(abs, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6bacb7b-8359-4561-8422-e500bdf80266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.105 909.4775\n"
     ]
    }
   ],
   "source": [
    "o = 0\n",
    "abs =0\n",
    "for i in df_v.loc[:,'abs']:\n",
    "    # print(i)\n",
    "    abs += (len(i))\n",
    "for i in df_v.loc[:,'original']:\n",
    "    o += (len(i))\n",
    "abs = abs/len(df_v)\n",
    "o /= len(df_v)\n",
    "print(abs, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143d68da-a260-4181-9d80-29a4804ed80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.79533333333333 844.8179259259259\n"
     ]
    }
   ],
   "source": [
    "o = 0\n",
    "abs =0\n",
    "for i in df_t.loc[:,'ext']:\n",
    "    # print(i)\n",
    "    abs += (len(i))\n",
    "for i in df_t.loc[:,'original']:\n",
    "    o += (len(i))\n",
    "abs = abs/len(df_t)\n",
    "o /= len(df_t)\n",
    "print(abs, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c17f9b4-cba0-42a1-b9d8-d8a51ba85b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244.99875 26.947481481481482\n"
     ]
    }
   ],
   "source": [
    "o = 0\n",
    "abs =0\n",
    "for i in df_v.loc[:,'ext']:\n",
    "    # print(i)\n",
    "    abs += (len(i))\n",
    "for i in df_v.loc[:,'original']:\n",
    "    o += (len(i))\n",
    "abs = abs/len(df_v)\n",
    "o /= len(df_t)\n",
    "print(abs, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37676725-b7c8-488d-b9da-922cce3d1505",
   "metadata": {},
   "source": [
    "abs, ext 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae52c91-d65f-49bf-8e81-8e04ae7f3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, s_type):\n",
    "    df = data.loc[:,['id','original', s_type]]\n",
    "    df.rename(columns = {s_type : 'summary'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eeee4e0-287a-45f1-bda5-1b8cc6c5b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abs = data_split(df_t,'abs')\n",
    "valid_abs = data_split(df_v,'abs')\n",
    "trian_ext = data_split(df_t,'ext')\n",
    "valid_ext = data_split(df_v,'ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63fcf4e2-7efe-4524-a3a7-f6b41a5e24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "class KoBARTSummaryDataset(Dataset):\n",
    "    def __init__(self, df,  pad_index = None, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tok = AutoTokenizer.from_pretrained('hyunwoongko/kobart', use_fast=True)\n",
    "        self.max_len = 1024\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "        if pad_index is None:\n",
    "            self.pad_index = self.tok.pad_token_id\n",
    "        else:\n",
    "            self.pad_index = pad_index\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.df.iloc[idx]\n",
    "        input_ids = self.tok.encode('<s> '+instance['original'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tok.encode(instance['summary'])\n",
    "        label_ids.append(self.tok.eos_token_id)\n",
    "        dec_input_ids = [self.tok.bos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1849c7-2d1c-43dd-adbe-f4ac90967af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "EVAL_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8bdd4b3-9140-4326-af44-a7c8c56697eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138359/3789152189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mext_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBARTSummaryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrian_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mext_valid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBARTSummaryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mabs_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBARTSummaryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_abs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mabs_valid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBARTSummaryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_abs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_138359/1666887452.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, pad_index, ignore_index)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpad_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hyunwoongko/kobart'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "ext_train_dataset = KoBARTSummaryDataset(trian_ext)\n",
    "ext_valid_dataset = KoBARTSummaryDataset(valid_ext)\n",
    "abs_train_dataset = KoBARTSummaryDataset(train_abs)\n",
    "abs_valid_dataset = KoBARTSummaryDataset(valid_abs)\n",
    "\n",
    "ext_train_loader = DataLoader(ext_train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n",
    "ext_valid_loader = DataLoader(ext_valid_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=True,num_workers=0)\n",
    "abs_train_loader = DataLoader(abs_train_dataset, batch_size=TRAIN_BATCH_SIZE ,shuffle=True,num_workers=0)\n",
    "abs_valid_loader = DataLoader(abs_valid_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6241888-535c-4edc-a98f-b0a1ce803afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Hitrate(y_true, y_pred):\n",
    "    m = datasets.load_metric('rouge')\n",
    "    rouge = m.compute(predictions= y_pred,references=y_true)\n",
    "    score = (rouge['rouge1'].mid.fmeasure+rouge['rouge2'].mid.fmeasure + rouge['rougeL'].mid.fmeasure) /3 \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76aee982-80e4-4908-af7c-593badf57b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int, verbose: bool, logger:logging.RootLogger=None)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.logger = logger\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\n",
    "\n",
    "        Args:\n",
    "            loss (float):\n",
    "\n",
    "        Examples:\n",
    "            \n",
    "        Note:\n",
    "            \n",
    "        \"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.save_model = True\n",
    "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55e97a20-e71d-4f5a-b787-1ee4d85640f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \n",
    "    Attributes:\n",
    "        model (`model`)\n",
    "        device (str)\n",
    "        loss_fn (Callable)\n",
    "        metric_fn (Callable)\n",
    "        optimizer (`optimizer`)\n",
    "        scheduler (`scheduler`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model,device,metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        # self.loss_fn = loss_fn\n",
    "        self.tok =  AutoTokenizer.from_pretrained('hyunwoongko/kobart', use_fast=True)\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.logger = logger\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.train_total_loss = 0\n",
    "        total_loss = 0\n",
    "        # pred_lst = []\n",
    "        # target_lst = []\n",
    "        for batch_index, data in enumerate(tqdm(dataloader)):\n",
    "            attention_mask = data['input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "            decoder_attention_mask = data['decoder_input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "            outputs = self.model(input_ids=data['input_ids'].to(device),\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  decoder_input_ids=data['decoder_input_ids'].to(device),\n",
    "                                  decoder_attention_mask=decoder_attention_mask,\n",
    "                                  labels=data['labels'].to(device), return_dict=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            self.train_total_loss += outputs.loss\n",
    "            total_loss += outputs.loss\n",
    "            outputs.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            if (batch_index+1) % 200 == 0:\n",
    "                print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}'.format\n",
    "                      (epoch_index , epoch_index, (batch_index+1) , total_loss / 200))\n",
    "\n",
    "                total_loss = 0\n",
    "                \n",
    "        self.train_mean_loss = self.train_total_loss / len(dataloader)\n",
    "    \n",
    "        msg = f'Epoch {epoch_index}, Train, loss: {self.train_mean_loss}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.val_score_all = 0\n",
    "        pred_lst = []\n",
    "        target_lst = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, data in enumerate(tqdm(dataloader)):\n",
    "                attention_mask = data['input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "                decoder_attention_mask = data['decoder_input_ids'].ne(self.tok.pad_token_id).float().to(device)\n",
    "                outputs = self.model.generate(input_ids=data['input_ids'].to(device),\n",
    "                                  attention_mask = attention_mask,\n",
    "                                     num_beams=5,\n",
    "                                     no_repeat_ngram_size=4,\n",
    "                                     decoder_start_token_id=self.tok.bos_token_id,\n",
    "                                     temperature=1.0, top_k=0, top_p=0.92,\n",
    "                                     length_penalty=1.0, min_length=1,\n",
    "                                     max_length=100,\n",
    "                                     early_stopping=False,\n",
    "                                     num_return_sequences=1,\n",
    "                                     do_sample= True).to(device)\n",
    "                # outputs = self.model(input_ids=data['input_ids'].to(device),\n",
    "                #                   attention_mask=attention_mask,\n",
    "                #                   decoder_input_ids=data['decoder_input_ids'].to(device),\n",
    "                #                   decoder_attention_mask=decoder_attention_mask,\n",
    "                #                   labels=data['labels'].to(device), return_dict=True)\n",
    "                ref = self.tok.batch_decode(\n",
    "                        data['decoder_input_ids'],\n",
    "                        skip_special_tokens=True\n",
    "                )\n",
    "                pred = self.tok.batch_decode(\n",
    "                        outputs,\n",
    "                        skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                # loss = self.loss_fn(sent_score, target)\n",
    "                # self.val_total_loss += outputs.loss\n",
    "            # self.val_mean_loss = self.val_total_loss / len(dataloader)\n",
    "                self.val_score = self.metric_fn(y_true=ref, y_pred=pred)\n",
    "                self.val_score_all += self.val_score\n",
    "            self.val_mean_score = self.val_score_all/len(dataloader)\n",
    "            msg = f'Epoch {epoch_index}, Validation, score : {self.val_mean_score}'\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1ae54d7-1c95-4730-99a0-682b28b47999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "ext_epochs = 1\n",
    "abs_epochs = 10\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.00001\n",
    "NUM_WORKERS = 1\n",
    "EARLY_STOPPING_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82ad566e-aa62-4f87-849f-7ca0e4fd2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dabd8408-efd1-495c-9daf-1d1fdc24b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_kobart_for_conditional_generation().to(device)\n",
    "# MODEL_DIR = os.path.join(ROOT_DIR, 'best3.pt')\n",
    "# model = get_kobart_for_conditional_generation().to(device)\n",
    "# model.load_state_dict(torch.load(MODEL_DIR)['model_state_dict'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, div_factor=1e3, max_lr=3e-5, epochs=ext_epochs, steps_per_epoch=len(ext_train_loader))\n",
    "metric_fn = Hitrate\n",
    "# torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99db7213-fa4b-43cf-998a-048637ecf8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model ,device, metric_fn,optimizer,scheduler)\n",
    "# early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "455b4570-c302-4ecf-92a5-810e2823294d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dfae0ea78043b2b1306082e47d6170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e5eb974440432db89c160635c4d378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1907: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1907: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation, score : 0.469955755939063\n",
      "best_ext.pt saved  0 0.469955755939063\n",
      "train finished, best2.pt saved.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "criterion = 0\n",
    "\n",
    "for epoch_index in tqdm(range(ext_epochs)):\n",
    "    \n",
    "    # trainer.train_epoch(ext_train_loader, epoch_index=epoch_index)\n",
    "    trainer.validate_epoch(ext_valid_loader, epoch_index=epoch_index)\n",
    "   \n",
    "    # early_stopping check\n",
    "    # early_stopper.check_early_stopping(loss=trainer.val_mean_score)\n",
    "\n",
    "    # if early_stopper.stop:\n",
    "    #     print('Early stopped')\n",
    "    #     break\n",
    "\n",
    "    if trainer.val_mean_score > criterion:\n",
    "        criterion = trainer.val_mean_score\n",
    "        torch.save({\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trainer.val_mean_score,\n",
    "            }, os.path.join(ROOT_DIR, 'law.pt'))\n",
    "        print('best_ext.pt saved ', epoch_index, trainer.val_mean_score)\n",
    "        \n",
    "        \n",
    "print(\"train finished, best2.pt saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c00b2969-631d-4bae-856f-44fdf668361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_DIR = os.path.join(ROOT_DIR, 'best3.pt')\n",
    "# model = get_kobart_for_conditional_generation().to(device)\n",
    "# model.load_state_dict(torch.load(MODEL_DIR)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73ccaf5d-797a-46ef-9689-9b2281992e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9e416ad890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = torch.nn.BCELoss(reduction='none')\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, div_factor=1e3, max_lr=3e-5, epochs=abs_epochs, steps_per_epoch=len(abs_train_loader))\n",
    "torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "084ed56f-4388-47e6-8b2b-beba57e5898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model ,device, metric_fn,optimizer, scheduler)\n",
    "# Set earlystopper\n",
    "# early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8feb20-3213-487a-967a-417b2839f6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_137564/3720604291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "criterion = 0\n",
    "\n",
    "for epoch_index in tqdm(range(abs_epochs)):\n",
    "    \n",
    "    trainer.train_epoch(abs_train_loader, epoch_index=epoch_index)\n",
    "    trainer.validate_epoch(abs_valid_loader, epoch_index=epoch_index)\n",
    "   \n",
    "    # early_stopping check\n",
    "#     early_stopper.check_early_stopping(loss=trainer.val_mean_score)\n",
    "\n",
    "#     if early_stopper.stop:\n",
    "#         print('Early stopped')\n",
    "#         break\n",
    "\n",
    "    if trainer.val_mean_score > criterion:\n",
    "        criterion = trainer.val_mean_score     \n",
    "        torch.save({\n",
    "            'epoch': epoch_index,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trainer.val_mean_score,\n",
    "            }, os.path.join(ROOT_DIR, 'final_abs.pt'))\n",
    "        print('final_abs.pt saved ', epoch_index, trainer.val_mean_score)\n",
    "        \n",
    "        \n",
    "print(\"train finished, best.pt saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357840-0629-44c6-8777-611aa5b38d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL_DIR = os.path.join(ROOT_DIR, 'best_abs.pt')\n",
    "# model = KoBARTConditionalGeneration().to(device)\n",
    "# model.load_state_dict(torch.load(MODEL_DIR)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053e3d4-e0c9-42ca-a57e-abf6498b340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# loss_fn = torch.nn.BCELoss(reduction='none')\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.2, div_factor=1e3, max_lr=0.0001, epochs=7, steps_per_epoch=len(abs_train_loader))\n",
    "# torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4833e-daac-476e-a8bb-cc7454ad4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, device, optimizer, scheduler)\n",
    "# # Set earlystopper\n",
    "# early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb1317-228a-4298-bc13-175c02a07b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TRAIN\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# criterion = 99999999\n",
    "\n",
    "# for epoch_index in tqdm(range(7)):\n",
    "    \n",
    "#     trainer.train_epoch(abs_train_loader, epoch_index=epoch_index)\n",
    "#     trainer.validate_epoch(abs_valid_loader, epoch_index=epoch_index)\n",
    "   \n",
    "#     # early_stopping check\n",
    "#     early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "#     if early_stopper.stop:\n",
    "#         print('Early stopped')\n",
    "#         break\n",
    "\n",
    "#     if trainer.val_mean_loss < criterion:\n",
    "#         criterion = trainer.val_mean_loss\n",
    "        \n",
    "#         torch.save({\n",
    "#             'epoch': epoch_index,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': trainer.val_mean_loss,\n",
    "#             }, os.path.join(ROOT_DIR, 'best_abs.pt'))\n",
    "#         print('best_abs.pt saved ', epoch_index, trainer.val_mean_loss)\n",
    "        \n",
    "        \n",
    "# print(\"train finished, best.pt saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c8cb8-256f-42f5-b373-1f4a8ce50b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
